### TÃ³m táº¯t  
BÃ i giáº£ng nÃ y giá»›i thiá»‡u tá»•ng quan vá» kiáº¿n trÃºc Transformer â€“ ná»n táº£ng chÃ­nh cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) hiá»‡n Ä‘áº¡i nhÆ° GPT. Transformer Ä‘Æ°á»£c giá»›i thiá»‡u láº§n Ä‘áº§u trong bÃ i bÃ¡o â€œAttention is All You Needâ€ nÄƒm 2017 vÃ  Ä‘Ã£ táº¡o ra bÆ°á»›c Ä‘á»™t phÃ¡ lá»›n trong cÃ¡c nhiá»‡m vá»¥ dá»‹ch mÃ¡y, chuyá»ƒn Ä‘á»•i ngÃ´n ngá»¯. Kiáº¿n trÃºc nÃ y bao gá»“m hai thÃ nh pháº§n chÃ­nh: bá»™ mÃ£ hÃ³a (encoder) vÃ  bá»™ giáº£i mÃ£ (decoder). QuÃ¡ trÃ¬nh hoáº¡t Ä‘á»™ng cá»§a Transformer báº¯t Ä‘áº§u báº±ng viá»‡c tÃ¡ch cÃ¢u thÃ nh cÃ¡c token, Ã¡nh xáº¡ cÃ¡c token nÃ y thÃ nh cÃ¡c vector embedding trong khÃ´ng gian Ä‘a chiá»u Ä‘á»ƒ biá»ƒu diá»…n Ã½ nghÄ©a ngá»¯ cáº£nh. Bá»™ decoder sau Ä‘Ã³ sá»­ dá»¥ng cÃ¡c vector nÃ y cÃ¹ng vá»›i pháº§n vÄƒn báº£n Ä‘áº§u ra táº¡m thá»i Ä‘á»ƒ dá»± Ä‘oÃ¡n tá»«ng tá»« tiáº¿p theo trong cÃ¢u dá»‹ch.  

![Parameters](/images/transformers-lec4.png)

BÃ i giáº£ng cÅ©ng giáº£i thÃ­ch khÃ¡i niá»‡m cÆ¡ cháº¿ tá»± chÃº Ã½ (self-attention), cho phÃ©p mÃ´ hÃ¬nh xÃ¡c Ä‘á»‹nh má»©c Ä‘á»™ quan trá»ng cá»§a tá»«ng tá»« trong cÃ¢u, giÃºp náº¯m báº¯t cÃ¡c má»‘i quan há»‡ dÃ i háº¡n giá»¯a cÃ¡c tá»« trong ngá»¯ cáº£nh.  

NgoÃ i ra, bÃ i giáº£ng phÃ¢n biá»‡t rÃµ rÃ ng giá»¯a Transformer vÃ  LLM, cÅ©ng nhÆ° cÃ¡c biáº¿n thá»ƒ sau nÃ y cá»§a Transformer nhÆ° BERT vÃ  GPT. BERT sá»­ dá»¥ng bá»™ mÃ£ hÃ³a vÃ  táº­p trung vÃ o viá»‡c dá»± Ä‘oÃ¡n cÃ¡c tá»« bá»‹ áº©n trong cÃ¢u báº±ng cÃ¡ch xem xÃ©t cáº£ ngá»¯ cáº£nh hai chiá»u (trÃ¡i vÃ  pháº£i), ráº¥t phÃ¹ há»£p cho cÃ¡c tÃ¡c vá»¥ nhÆ° phÃ¢n tÃ­ch cáº£m xÃºc. NgÆ°á»£c láº¡i, GPT chá»‰ sá»­ dá»¥ng bá»™ giáº£i mÃ£ vÃ  dá»± Ä‘oÃ¡n tá»« tiáº¿p theo theo hÆ°á»›ng tá»« trÃ¡i sang pháº£i, phÃ¹ há»£p cho viá»‡c táº¡o vÄƒn báº£n tuáº§n tá»±.  

Cuá»‘i cÃ¹ng, bÃ i giáº£ng nháº¥n máº¡nh ráº±ng khÃ´ng pháº£i táº¥t cáº£ cÃ¡c Transformer Ä‘á»u lÃ  LLM vÃ  cÅ©ng khÃ´ng pháº£i táº¥t cáº£ LLM Ä‘á»u dá»±a trÃªn Transformer, vÃ¬ trÆ°á»›c Ä‘Ã¢y cÃ²n cÃ³ cÃ¡c mÃ´ hÃ¬nh máº¡ng nÆ¡-ron há»“i tiáº¿p (RNN), máº¡ng bá»™ nhá»› dÃ i-ngáº¯n háº¡n (LSTM) vÃ  cÃ¡c kiáº¿n trÃºc CNN cÅ©ng cÃ³ thá»ƒ thá»±c hiá»‡n cÃ¡c tÃ¡c vá»¥ mÃ´ hÃ¬nh ngÃ´n ngá»¯.  

### Äiá»ƒm nháº¥n quan trá»ng  
- ğŸ”‘ Transformer lÃ  kiáº¿n trÃºc ná»n táº£ng cá»§a háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n hiá»‡n Ä‘áº¡i.  
- ğŸ§© QuÃ¡ trÃ¬nh tiá»n xá»­ lÃ½ bao gá»“m token hÃ³a vÃ  Ã¡nh xáº¡ token thÃ nh vector embedding Ä‘á»ƒ biá»ƒu diá»…n ngá»¯ nghÄ©a.  
- ğŸ” CÆ¡ cháº¿ tá»± chÃº Ã½ (self-attention) lÃ  Ä‘iá»ƒm máº¥u chá»‘t giÃºp mÃ´ hÃ¬nh hiá»ƒu Ä‘Æ°á»£c má»‘i quan há»‡ dÃ i háº¡n giá»¯a cÃ¡c tá»« trong cÃ¢u.  
- ğŸ”„ Transformer bao gá»“m hai thÃ nh pháº§n chÃ­nh: encoder vÃ  decoder, nhÆ°ng GPT chá»‰ cÃ³ decoder, BERT chá»‰ cÃ³ encoder.  
- ğŸ§  BERT lÃ  mÃ´ hÃ¬nh hai chiá»u, hiá»‡u quáº£ trong cÃ¡c nhiá»‡m vá»¥ nhÆ° phÃ¢n tÃ­ch cáº£m xÃºc nhá» kháº£ nÄƒng dá»± Ä‘oÃ¡n tá»« bá»‹ áº©n trong ngá»¯ cáº£nh.  
- âš™ï¸ GPT dá»± Ä‘oÃ¡n tá»« tiáº¿p theo theo trÃ¬nh tá»± tá»« trÃ¡i sang pháº£i, Ã¡p dá»¥ng cho viá»‡c sinh vÄƒn báº£n.  
- ğŸŒ KhÃ´ng pháº£i táº¥t cáº£ Transformers Ä‘á»u lÃ  LLM vÃ  khÃ´ng pháº£i táº¥t cáº£ LLM Ä‘á»u dá»±a trÃªn Transformer; cÃ³ cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhÆ° RNN, LSTM vÃ  CNN cÅ©ng cÃ³ thá»ƒ lÃ  LLM.  

### Nhá»¯ng hiá»ƒu biáº¿t quan trá»ng  
- ğŸ“œ **Sá»± phÃ¡t triá»ƒn cá»§a Transformer tá»« bÃ i bÃ¡o â€œAttention is All You Needâ€**: BÃ i bÃ¡o nÄƒm 2017 Ä‘Ã£ má»Ÿ ra ká»· nguyÃªn má»›i cho cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯, thay tháº¿ cÃ¡c mÃ´ hÃ¬nh RNN truyá»n thá»‘ng báº±ng kiáº¿n trÃºc dá»±a trÃªn attention, giÃºp xá»­ lÃ½ song song hiá»‡u quáº£ vÃ  náº¯m báº¯t Ä‘Æ°á»£c cÃ¡c má»‘i quan há»‡ ngá»¯ cáº£nh dÃ i háº¡n. Äiá»u nÃ y lÃ m cho cÃ¡c mÃ´ hÃ¬nh nhÆ° GPT vÃ  BERT cÃ³ thá»ƒ Ä‘áº¡t hiá»‡u suáº¥t cá»±c ká»³ cao trong nhiá»u nhiá»‡m vá»¥ NLP.  

- ğŸ§® **Token hÃ³a vÃ  vector embedding giáº£i quyáº¿t bÃ i toÃ¡n biá»ƒu diá»…n ngá»¯ cáº£nh**: Viá»‡c chuyá»ƒn Ä‘á»•i cÃ¢u thÃ nh cÃ¡c token vÃ  Ã¡nh xáº¡ token thÃ nh vector trong khÃ´ng gian Ä‘a chiá»u cho phÃ©p mÃ´ hÃ¬nh hiá»ƒu Ä‘Æ°á»£c má»‘i liÃªn há»‡ ngá»¯ nghÄ©a giá»¯a cÃ¡c tá»«, vÃ­ dá»¥ nhÆ° â€œkingâ€, â€œmanâ€, â€œwomanâ€ cÃ³ vector gáº§n nhau ngá»¯ nghÄ©a hÆ¡n so vá»›i â€œkingâ€ vÃ  má»™t loáº¡i trÃ¡i cÃ¢y. ÄÃ¢y lÃ  bÆ°á»›c ná»n táº£ng Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn hiá»‡u quáº£.  

- ğŸ”„ **PhÃ¢n biá»‡t rÃµ rÃ ng giá»¯a encoder vÃ  decoder trong Transformer**: Encoder chá»‹u trÃ¡ch nhiá»‡m mÃ£ hÃ³a thÃ´ng tin Ä‘áº§u vÃ o thÃ nh embedding, cÃ²n decoder dá»±a vÃ o embedding vÃ  thÃ´ng tin Ä‘áº§u ra táº¡m thá»i Ä‘á»ƒ sinh káº¿t quáº£. Äiá»u nÃ y táº¡o nÃªn sá»± linh hoáº¡t trong cÃ¡c á»©ng dá»¥ng khÃ¡c nhau, vÃ­ dá»¥ nhÆ° dá»‹ch mÃ¡y, táº¡o vÄƒn báº£n, hay hoÃ n thiá»‡n cÃ¢u.  

- ğŸ‘ï¸â€ğŸ—¨ï¸ **CÆ¡ cháº¿ self-attention cho phÃ©p mÃ´ hÃ¬nh xá»­ lÃ½ phá»¥ thuá»™c dÃ i háº¡n**: Má»™t trong nhá»¯ng Ä‘iá»ƒm máº¡nh lá»›n nháº¥t cá»§a Transformer chÃ­nh lÃ  kháº£ nÄƒng cÃ¢n nháº¯c táº¥t cáº£ cÃ¡c tá»« trong cÃ¢u (hoáº·c Ä‘oáº¡n vÄƒn) khi dá»± Ä‘oÃ¡n tá»« tiáº¿p theo, khÃ´ng bá»‹ giá»›i háº¡n bá»Ÿi khoáº£ng cÃ¡ch tá»«, giÃºp mÃ´ hÃ¬nh duy trÃ¬ ngá»¯ cáº£nh má»™t cÃ¡ch toÃ n diá»‡n vÃ  chÃ­nh xÃ¡c hÆ¡n.  

- âš–ï¸ **Sá»± khÃ¡c biá»‡t trong cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a BERT vÃ  GPT**: BERT dá»± Ä‘oÃ¡n cÃ¡c tá»« bá»‹ áº©n trong cÃ¢u dá»±a trÃªn ngá»¯ cáº£nh hai chiá»u nÃªn phÃ¹ há»£p vá»›i cÃ¡c tÃ¡c vá»¥ nhÆ° phÃ¢n tÃ­ch cáº£m xÃºc, hiá»ƒu ngá»¯ nghÄ©a sÃ¢u sáº¯c. CÃ²n GPT sinh tá»« theo cÃ¡ch tuáº§n tá»±, ráº¥t thÃ­ch há»£p cho viá»‡c táº¡o ná»™i dung hoáº·c tráº£ lá»i cÃ¢u há»i. Viá»‡c hiá»ƒu rÃµ sá»± khÃ¡c biá»‡t nÃ y giÃºp lá»±a chá»n mÃ´ hÃ¬nh phÃ¹ há»£p cho tá»«ng nhiá»‡m vá»¥ cá»¥ thá»ƒ.  

- ğŸŒ **Transformer khÃ´ng chá»‰ dÃ nh cho ngÃ´n ngá»¯**: Transformer cÃ²n Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c lÄ©nh vá»±c khÃ¡c nhÆ° thá»‹ giÃ¡c mÃ¡y tÃ­nh vá»›i Vision Transformers (ViT), giÃºp phÃ¢n loáº¡i áº£nh, phÃ¡t hiá»‡n dá»‹ váº­t trÃªn Ä‘Æ°á»ng, phÃ¢n loáº¡i khá»‘i u,... Äiá»u nÃ y cho tháº¥y tÃ­nh Ä‘a dá»¥ng vÃ  máº¡nh máº½ cá»§a kiáº¿n trÃºc Transformer vÆ°á»£t ra ngoÃ i giá»›i háº¡n xá»­ lÃ½ ngÃ´n ngá»¯.  

- ğŸ”„ **KhÃ´ng Ä‘á»“ng nháº¥t giá»¯a Transformer vÃ  LLM**: Hiá»ƒu ráº±ng Transformer lÃ  má»™t kiáº¿n trÃºc máº¡ng nÆ¡-ron trong khi LLM lÃ  má»™t khÃ¡i niá»‡m rá»™ng hÆ¡n vá» mÃ´ hÃ¬nh xá»­ lÃ½ ngÃ´n ngá»¯, cÃ³ thá»ƒ dá»±a trÃªn nhiá»u kiáº¿n trÃºc khÃ¡c nhau nhÆ° RNN, LSTM, CNN. Viá»‡c khÃ´ng nháº§m láº«n hai khÃ¡i niá»‡m nÃ y ráº¥t quan trá»ng Ä‘á»ƒ cÃ³ cÃ¡i nhÃ¬n Ä‘Ãºng Ä‘áº¯n vá» cÃ´ng nghá»‡ vÃ  lá»±a chá»n phÆ°Æ¡ng phÃ¡p phÃ¹ há»£p khi phÃ¡t triá»ƒn hoáº·c Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh.  
